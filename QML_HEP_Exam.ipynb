{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EA2HKiVQy9KY",
        "notebookRunGroups": {
          "groupValue": "2"
        }
      },
      "source": [
        "# Task 1 - Installing Requirements (Run before Task-1 Required Imports)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Do0OF7z3rPVq",
        "outputId": "bb5b58c2-e0d8-4e39-ea2e-d40109435ead"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet cirq\n",
        "!pip install qiskit\n",
        "!pip install pylatexenc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BNtadaxirg53"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.meta_path[:] = [f for f in sys.meta_path if \"DaskFinder\" not in str(f)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cv7PhaH7r3WL"
      },
      "source": [
        "I am using the above cell as I am getting issues with the DaskFinder when importing cirq into google colab. It is my request to make sure to run this cell befor importing cirq if this is an issue in anyone's device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CClBiUR3yPMD"
      },
      "source": [
        "# Task 1 - Required Imports (Run Before Task-1 Part-1 and 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0JkmYs-yXyC"
      },
      "source": [
        "**Cirq:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KBujERRwr_oi"
      },
      "outputs": [],
      "source": [
        "import cirq\n",
        "import cirq_google\n",
        "from cirq.contrib.svg import SVGCircuit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gByKPJsqygR9"
      },
      "source": [
        "**Qiskit:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VfzF051gyWuR"
      },
      "outputs": [],
      "source": [
        "from qiskit.circuit import QuantumCircuit, Parameter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNrx7Vqbyz9E"
      },
      "source": [
        "**Misc:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "B3qJP07Cyzhq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pylatexenc\n",
        "import matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aH9dGoCHxbau"
      },
      "source": [
        "# Task 1 - Part 1 (Using Cirq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3EXs1b2qseLZ"
      },
      "outputs": [],
      "source": [
        "qubits=[cirq.LineQubit(i) for i in range(5)] #I am using line qubits. Here I am creating them and storing them in a list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "OtZ_kQvEs44R",
        "outputId": "54a10ef7-bd52-4ef3-ad8f-596269b4394b"
      },
      "outputs": [],
      "source": [
        "circuit=cirq.Circuit() #creating empty circuit\n",
        "circuit.append(cirq.H.on_each(qubits))               #Applying hadamard on all of them\n",
        "\n",
        "circuit.append([cirq.CNOT(qubits[0],qubits[1]),      #Here I am appending a list of CNOT gate objects to the circuit. Each CNOT gate has first qubit controla and second as target\n",
        "                cirq.CNOT(qubits[1],qubits[2]),\n",
        "                cirq.CNOT(qubits[2],qubits[3]),\n",
        "                cirq.CNOT(qubits[3],qubits[4])])\n",
        "\n",
        "circuit.append(cirq.SWAP(qubits[0],qubits[4]))       #This is the swap gate for first and last qubit\n",
        "\n",
        "circuit.append(cirq.rx(np.pi/2)(qubits[3]))          #Here I am applying the rx gate i.e. rotate about x-axis gate.\n",
        "\n",
        "SVGCircuit(circuit)                                  #I have used SVGCircuit for showing a good picture of the circuit. However print(circuit) would also give an accurate representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDqJZJpzzQ43"
      },
      "source": [
        "# Task 1- Part 2 (Using Qiskit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTt4xqzBzn7U",
        "outputId": "a46a58e6-5535-4a7c-eb89-544822050245"
      },
      "outputs": [],
      "source": [
        "circuit=QuantumCircuit(5,1)\n",
        "circuit.h([0,2,3,4])      #Applying hadamard to first, third and fourth qubit. We laso applied it to the 5th qubit as we are using that as the ancilla in our swap test\n",
        "circuit.rx(np.pi/3,1)     #rotating second qubit by pi/3 around X\n",
        "circuit.cswap(4,0,2)      #Fredkin on 1st and 3rd with last qubit as control\n",
        "circuit.cswap(4,1,3)      #Fredkin on 2nd and 4th with last qubit as control\n",
        "circuit.h(4)              #Applying hadamard on ancilla again\n",
        "circuit.measure(4,0)      #measuring ancilla (Greater probability of 0 means greater similarity. We can find probability through a sampler)\n",
        "print(circuit)\n",
        "#Here we can also use circuit.draw('mpl') for a beautified version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xqJ17-t5ovO"
      },
      "source": [
        "# Task-2 Installing Requirements (Run before going to Task 2- Required Imports)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "oOzFo-xx6XMr",
        "outputId": "53342e96-238b-4c07-d477-2a2b579641e9"
      },
      "outputs": [],
      "source": [
        "!pip install energyflow           #for loading quark-gluon data\n",
        "!pip install torch\n",
        "!pip install torch-geometric\n",
        "!pip install torch_cluster        #this might take 5-6 mins to install. Ateast that was the case on my device\n",
        "!pip install scikit-learn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSaM7YUJ7WFT"
      },
      "source": [
        "# Task-2 Required Imports (Run before running the Task-2 Graph Based ...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNGfgocI7aYy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import global_mean_pool, global_max_pool, GATConv, EdgeConv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch_cluster import knn_graph\n",
        "import energyflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfuqcqlx8fQO"
      },
      "source": [
        "# Task-2 Graph Based Quark-Gluon Classification Architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLqE2Sg_edyz"
      },
      "source": [
        "The objective of task-2 is to explore two graph based architectures for classification of quarks and gluon jets in the dataset provided in the task.\n",
        "\n",
        "I have chosen the following graph based architectures:\n",
        "\n",
        "\n",
        "1.   Graph Attention Networks (**GAT**)\n",
        "2.   Dynamic Graph Convolutional Neural Networks (**DGCNN**)\n",
        "\n",
        "\n",
        "**Before I explore these models, I would like to explain how I projected this dataset to a set of interconnected nodes and edges-**\n",
        "\n",
        "Each jet in the dataset has been considered as a graph (i.e set of interconnected nodes and edges). There are 100,000 jets in total. Thus we would have 100,000 graphs.\n",
        "\n",
        "Every jet is a 2-D array in the dataset. It is an array of particles. Every particle in the jet is a 1-D array consisting of 4 features (pt, rapidity, azimuthal angle, and pdgid)\n",
        "\n",
        "Thus I am considering every particle in the jet as a node inside the graph. The features are basically attributes of that node. Since features can vary over several orders of magnitude, I also normalize each feature which helps in stabilizing training and ensures that all features contribute comparably when computing distances. The dataset is also padded with zeros as particles for jets that have lesser than the maximum number of particles. However, these don't provide any information thus they are removed.\n",
        "\n",
        "Now coming to edges, I constructed edges for each node by using the k-Nearest Neighbors approach. For every node, I connected it to its k-NN based on euclidean distance in the feature space.  In the current case, using k=16 neighbors typically provides sufficient connectivity for the GAT or DGCNN to learn meaningful representations without overwhelming the model with too many edges.\n",
        "\n",
        "Also, in the k-NN graph, I have not included self-loops (edges from a node to itself) since they do not provide additional relational information.\n",
        "\n",
        "Additionaly, GAT uses a fixed (static) graph using the raw features. In case of DGCNN, the benefit is in dynamically recomputing the k-NN graph at each layer to capture evolving relationships as the node features get updated.\n",
        "\n",
        "Once the nodes are processed by the network layers, I have applied global pooling to aggregate node-level information into a single, fixed-size graph-level representation. This aggregated vector is used for the final classification task.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycMkTtCA5Hcq"
      },
      "source": [
        "The **create_graph_list** function below shows what I have written above.\n",
        "\n",
        "It loads the data set as two arrays-\n",
        "\n",
        "A 3-D array (X) of shape (N,M,d) where N is the number of jets, M is max number of particles per jet and d is the number of features per particle.\n",
        "\n",
        "A 1-D array (Y) of the output labels (Quark/Gluon) for every jet\n",
        "\n",
        "Then performs normalization, removing padding, k-NN graph finding.\n",
        "It returns a list of PyG objects which are basically jets wrapped up with their respective labels. Run the cell below to preprocess the dataset and get it into appropriate for running through the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A47s0gnU49De"
      },
      "outputs": [],
      "source": [
        "def create_graph_list(k):\n",
        "\n",
        "    X,Y=energyflow.qg_jets.load(num_data=100000, pad=True, ncol=4, generator='pythia',with_bc=False, cache_dir='~/.energyflow')     #loading the QG dataset X is 3-D array and Y is 1-D\n",
        "\n",
        "    data_list=[]\n",
        "\n",
        "    for i in range(X.shape[0]):                                      #Iterating through the jets to create a graph for each jet\n",
        "        x=torch.tensor(X[i], dtype=torch.float)                      # shape: (M, d)\n",
        "        x=(x-x.mean(dim=0))/x.std(dim=0)                             #Normalizing the features across particles (for each feature column).\n",
        "        mask=(x.abs().sum(dim=1)>1e-8)                               #Removing padded particles (all input features 0)\n",
        "        x=x[mask]\n",
        "        edge_index=knn_graph(x,k=k,loop=False)                       #Finds k-NN for every node/particle in the graph/jet in feature space (uses Euclidean distance)\n",
        "        label=torch.tensor([Y[i]],dtype=torch.long)                  #I am wrapping the label as a tensor as well to add to the PyG data object\n",
        "        data_list.append(Data(x=x,edge_index=edge_index,y=label))    #creating a PyG data object and appending it to a list of graphs. Each graph/jet is a PyG object\n",
        "\n",
        "    return data_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SrMIYYO9dT_"
      },
      "source": [
        "**GAT:**\n",
        "\n",
        "GAT layers learn to weigh each neighbor’s influence differently by computing attention scores. Multiple attention heads allow the model to capture different types of interactions. This is why I have chose GAT as one of my architectures as not all particles contribute equally to identifying a jet as quark- or gluon-initiated. GAT’s attention mechanism lets the network focus more on the most informative particle interactions. The heads enable the network to learn various aspects of the relationships between particles, which is crucial given the complex structure of jets.\n",
        "\n",
        "\n",
        "After processing through three GAT layers, I have used global mean pooling to aggregate node features into a single graph-level feature, which is then fed into a fully connected layer for classification.\n",
        "\n",
        "Below is the implementation of the GAT. Please run the cell so that it can be used later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "A2wQgUxT8jH8",
        "outputId": "9eea2b6d-eb04-45c8-92e0-b3f85160fd76"
      },
      "outputs": [],
      "source": [
        "class QG_GAT(nn.Module):\n",
        "\n",
        "  def __init__(self, in_channels, hidden_channels, out_channels=2, heads=4):\n",
        "\n",
        "    super().__init__()\n",
        "    #3 processing layers\n",
        "    self.layer_1=GATConv(in_channels, hidden_channels, heads=heads, concat=True)             #'heads' attention heads and their outputs concatenated. Outputs are feature vectors of hidden channels\n",
        "    self.layer_2=GATConv(hidden_channels*heads, hidden_channels, heads=heads, concat=True)   #Thus input channels is hidden_channels*heads. Same heads, concatenation and output feature vector size as as before\n",
        "    self.layer_3=GATConv(hidden_channels*heads, hidden_channels, heads=1, concat=False)      #1 attention head only now therefore the fc layer gets a feature vector of hidden_channels size\n",
        "\n",
        "    self.fc=nn.Linear(hidden_channels, out_channels)                                         #maps to final outputs\n",
        "\n",
        "  def forward(self, x, edge_index, batch):\n",
        "\n",
        "    x=functional.elu(self.layer_1(x, edge_index))                                            #Applying GAT layers with ELU activation.\n",
        "    x=functional.elu(self.layer_2(x, edge_index))\n",
        "    x=functional.elu(self.layer_3(x, edge_index))\n",
        "\n",
        "    x=global_mean_pool(x, batch)\n",
        "\n",
        "    return self.fc(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu-MHVwe_QiR"
      },
      "source": [
        "**DGCNN-**\n",
        "\n",
        "Instead of using a fixed graph, DGCNN recomputes the graph (using k-NN) at every layer. This means that after each layer, as the node features change, the graph’s connectivity is updated accordingly. Thus, I used DGCNN the network learns, the optimal relationships between particles can change. DGCNN’s dynamic graph construction allows the network to update these relationships at every layer, capturing higher-level, context-dependent interactions.\n",
        "\n",
        "DGCNN uses EdgeConv layers that consider both a node and its neighbors. EdgeConv layers are particularly good at learning local structures, which is important because the spatial and energy distributions of particles in a jet are key to distinguishing between quark and gluon jets.\n",
        "\n",
        "After several EdgeConv layers, global pooling (both max and mean) aggregates the node features into a graph-level feature vector, which is then passed through a fully connected layer to yield class scores.\n",
        "\n",
        "\n",
        "Below is the implementation of the DGCNN. Please run the cell so that it can be used later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oo8CTrVkV1jm"
      },
      "outputs": [],
      "source": [
        "class QG_DGCNN(nn.Module):\n",
        "\n",
        "  def __init__(self, in_channels, hidden_channels, out_channels=2, k=16):\n",
        "\n",
        "    super().__init__()\n",
        "    self.k=k\n",
        "    #4 processing layers\n",
        "    self.conv_layer_1=EdgeConv(nn.Sequential(nn.Linear(2*in_channels, hidden_channels),nn.ReLU(), nn.Linear(hidden_channels,hidden_channels)))            #Input dimension is 2*in_channels because features of a node and its neighbor are concatenated. Outputs a feature vector of size hidden channels\n",
        "    self.conv_layer_2=EdgeConv(nn.Sequential(nn.Linear(2*hidden_channels, hidden_channels), nn.ReLU(), nn.Linear(hidden_channels,hidden_channels)))       #Input dimension is 2*hidden_channels (from previous layer's output)\n",
        "    self.conv_layer_3=EdgeConv(nn.Sequential(nn.Linear(2*hidden_channels, hidden_channels), nn.ReLU(), nn.Linear(hidden_channels,hidden_channels)))\n",
        "    self.conv_layer_4=EdgeConv(nn.Sequential(nn.Linear(2*hidden_channels, hidden_channels), nn.ReLU(), nn.Linear(hidden_channels,hidden_channels)))\n",
        "\n",
        "    self.conv_layer_list=[self.conv_layer_1, self.conv_layer_2, self.conv_layer_3, self.conv_layer_4]                                #Save all EdgeConv layers in a list for iteration in the forward pass\n",
        "\n",
        "    self.fc=nn.Linear(hidden_channels*2, out_channels)                                                                               #Takes the concatenated output from global max and mean pooling (2*hidden_channels) and maps it to out_channels classes\n",
        "\n",
        "\n",
        "  def forward(self, x, edge_index, batch):\n",
        "\n",
        "    for layer in self.conv_layer_list:\n",
        "      edge_index = knn_graph(x, k=self.k, batch=batch, loop=False)                 #Dynamically computing the k-NN graph based on the current node features\n",
        "      x=layer(x, edge_index)                                                       #Updating the node features using the current EdgeConv layer\n",
        "\n",
        "    x_max=global_max_pool(x, batch)\n",
        "    x_mean=global_mean_pool(x, batch)\n",
        "    x=torch.cat([x_max, x_mean], dim=1)                                            #Concatenating both pooled representations to form a graph-level feature vector\n",
        "\n",
        "    return self.fc(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-C_rh9O6CIBe"
      },
      "source": [
        "Below are functions for training and testing. These are used for a single epoch and are iteratively run later in main() for multiple (20) epochs\n",
        "\n",
        "Please run the cell so that they can be used later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBqpG_CsV1R-"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, loader, optimizer, device):\n",
        "    \n",
        "    model.train()                                                 #Putting model in train mode\n",
        "    total_loss=0\n",
        "    for batch in loader:                                          #going through the batches here\n",
        "        batch=batch.to(device)\n",
        "        optimizer.zero_grad()                                     #I am clearing all previous gradient computations here\n",
        "        forward=model(batch.x, batch.edge_index, batch.batch)     #Running the forward method in the given model here\n",
        "        loss=functional.cross_entropy(forward, batch.y)           #Calculating cross entropy loss\n",
        "        loss.backward()                                           #Backpropagation\n",
        "        optimizer.step()                                          #Updating the parameters\n",
        "        total_loss+=loss.item()*batch.num_graphs                  #Loss for each batch is weighted with the number of graphs in each batch\n",
        "    return total_loss/len(loader.dataset)                         #Returns the average loss per graph/jet for an epoch\n",
        "\n",
        "def test_epoch(model, loader, device):\n",
        "\n",
        "    model.eval()                                                  #Putting model in evaluation mode\n",
        "    correct=0\n",
        "    for batch in loader:\n",
        "        batch=batch.to(device)\n",
        "        with torch.no_grad():                                     #Disabling gradient computation for evaluation\n",
        "            forward=model(batch.x, batch.edge_index, batch.batch) #Running the forward method in the model here\n",
        "            prediction=forward.argmax(dim=1)                      #Predicted class\n",
        "            correct+=prediction.eq(batch.y).sum().item()          #Number of correctly predicted graphs\n",
        "    return correct/len(loader.dataset)                            #Returning accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_dZXGRaDJDN"
      },
      "source": [
        "Below is the main functions that trains and tests both these models on 'num_epochs' epochs. I am using 20 as num_epochs when I call main.\n",
        "\n",
        "Please run the cell to train, test and generate best accuracy values when predicting with both models.\n",
        "\n",
        "I only split the dataset into training and test sets for simplicity, but a validation set can be put in as well with a different split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "in7A8REnV7BC"
      },
      "outputs": [],
      "source": [
        "def main(num_epochs):\n",
        "\n",
        "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    #print(device) #Can be used for checking what device being used for training\n",
        "    data_list=create_graph_list(k=16)                                                   #I am using k as 16 for k-NN\n",
        "    train_data,test_data=train_test_split(data_list, test_size=0.2, random_state=42)    #I have used a 80:20 split for training and testing data\n",
        "\n",
        "    train_loader=DataLoader(train_data, batch_size=32, shuffle=True)                    #I am doing batch processing here\n",
        "    test_loader=DataLoader(test_data, batch_size=32, shuffle=False)                     #Each Batch contains 32 graphs/jets and training data batches have been shuffled\n",
        "\n",
        "    in_channels=data_list[0].x.shape[1]                                                 #Number of input freatures per particle/node (i.e. d in (N,M,d))\n",
        "\n",
        "    gat_model=QG_GAT(in_channels, hidden_channels=64, out_channels=2, heads=4).to(device)   #Instance of our Quark Gluon GAT Classifier\n",
        "    dgcnn_model=QD_DGCNN(in_channels, hidden_channels=64, out_channels=2, k=16).to(device)  #Instance of the DGCNN Classifier\n",
        "\n",
        "    optimizer=torch.optim.Adam(model.parameters(), lr=1e-3)                            #I am using ADAM for training with 10^-3 learning rate\n",
        "\n",
        "    gat_best_accuracy, dgcnn_best_accuracy=0,0                                         #Starting Training and Testing here over num_epochs\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        gat_train_loss = train_epoch(model, train_loader, optimizer, device)\n",
        "        gat_train_accuracy = test_epoch(model, train_loader, device)\n",
        "        gat_test_accuracy = test_epoch(model, test_loader, device)\n",
        "\n",
        "        dgcnn_train_loss = train_epoch(model, train_loader, optimizer, device)\n",
        "        dgcnn_train_accuracy = test_epoch(model, train_loader, device)\n",
        "        dgcnn_test_accuracy = test_epoch(model, test_loader, device)\n",
        "\n",
        "        if gat_test_accuracy>gat_best_accuracy:\n",
        "            gat_best_accuracy=gat_test_accuracy\n",
        "            torch.save(gat_model.state_dict(), 'best_gat_model.pt')                    #Saving the best model parameters for later use. I can load into the model if I wish to\n",
        "\n",
        "        if dgcnn_test_accuracy>dgcnn_best_accuracy:\n",
        "            dgcnn_best_accuracy=dgcnn_test_accuracy\n",
        "            torch.save(dgcnn_model.state_dict(), 'best_dgcnn_model.pt')\n",
        "\n",
        "        print(f\"Epoch: {epoch+1:02d}\")\n",
        "        print(f\"GAT Loss: {gat_train_loss:.4f}, GAT Train Accuaracy: {gat_train_accuracy:.4f}, GAT Test Accuracy: {gat_test_accuracy:.4f}\")\n",
        "        print(f\"DGCNN Loss: {dgcnn_train_loss:.4f}, DGCNN Train Accuaracy: {dgcnn_train_accuracy:.4f}, DGCNN Test Accuracy: {dgcnn_test_accuracy:.4f}\")\n",
        "        print()\n",
        "\n",
        "    print(f\"\\nTraining complete\")\n",
        "    print(\"Best GAT Accuracy:\", gat_best_accuracy)\n",
        "    print(\"Best DGCNN Accuracy:\", dgcnn_best_accuracy)\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    main(num_epochs=20)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "qml-hep",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
