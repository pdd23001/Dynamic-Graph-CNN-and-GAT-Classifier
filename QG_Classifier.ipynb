{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xqJ17-t5ovO"
      },
      "source": [
        "# Installing Requirements (Run before going to Required Imports)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "oOzFo-xx6XMr",
        "outputId": "53342e96-238b-4c07-d477-2a2b579641e9"
      },
      "outputs": [],
      "source": [
        "!pip install energyflow           #for loading quark-gluon data\n",
        "!pip install torch\n",
        "!pip install torch-geometric\n",
        "!pip install torch_cluster        #this might take 5-6 mins to install. Ateast that was the case on my device\n",
        "!pip install scikit-learn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSaM7YUJ7WFT"
      },
      "source": [
        "# Required Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oNGfgocI7aYy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import global_mean_pool, global_max_pool, GATConv, EdgeConv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch_cluster import knn_graph\n",
        "import energyflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfuqcqlx8fQO"
      },
      "source": [
        "# Graph Based Quark-Gluon Classification Architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLqE2Sg_edyz"
      },
      "source": [
        "The objective of task-2 is to explore two graph based architectures for classification of quarks and gluon jets in the dataset provided in the task.\n",
        "\n",
        "I have chosen the following graph based architectures:\n",
        "\n",
        "\n",
        "1.   Graph Attention Networks (**GAT**)\n",
        "2.   Dynamic Graph Convolutional Neural Networks (**DGCNN**)\n",
        "\n",
        "\n",
        "**Before I explore these models, I would like to explain how I projected this dataset to a set of interconnected nodes and edges-**\n",
        "\n",
        "Each jet in the dataset has been considered as a graph (i.e set of interconnected nodes and edges). There are 100,000 jets in total. Thus we would have 100,000 graphs.\n",
        "\n",
        "Every jet is a 2-D array in the dataset. It is an array of particles. Every particle in the jet is a 1-D array consisting of 4 features (pt, rapidity, azimuthal angle, and pdgid)\n",
        "\n",
        "Thus I am considering every particle in the jet as a node inside the graph. The features are basically attributes of that node. Since features can vary over several orders of magnitude, I also normalize each feature which helps in stabilizing training and ensures that all features contribute comparably when computing distances. The dataset is also padded with zeros as particles for jets that have lesser than the maximum number of particles. However, these don't provide any information thus they are removed.\n",
        "\n",
        "Now coming to edges, I constructed edges for each node by using the k-Nearest Neighbors approach. For every node, I connected it to its k-NN based on euclidean distance in the feature space.  In the current case, using k=16 neighbors typically provides sufficient connectivity for the GAT or DGCNN to learn meaningful representations without overwhelming the model with too many edges.\n",
        "\n",
        "Also, in the k-NN graph, I have not included self-loops (edges from a node to itself) since they do not provide additional relational information.\n",
        "\n",
        "Additionaly, GAT uses a fixed (static) graph using the raw features. In case of DGCNN, the benefit is in dynamically recomputing the k-NN graph at each layer to capture evolving relationships as the node features get updated.\n",
        "\n",
        "Once the nodes are processed by the network layers, I have applied global pooling to aggregate node-level information into a single, fixed-size graph-level representation. This aggregated vector is used for the final classification task.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycMkTtCA5Hcq"
      },
      "source": [
        "The **create_graph_list** function below shows what I have written above.\n",
        "\n",
        "It loads the data set as two arrays-\n",
        "\n",
        "A 3-D array (X) of shape (N,M,d) where N is the number of jets, M is max number of particles per jet and d is the number of features per particle.\n",
        "\n",
        "A 1-D array (Y) of the output labels (Quark/Gluon) for every jet\n",
        "\n",
        "Then performs normalization, removing padding, k-NN graph finding.\n",
        "It returns a list of PyG objects which are basically jets wrapped up with their respective labels. Run the cell below to preprocess the dataset and get it into appropriate for running through the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "A47s0gnU49De"
      },
      "outputs": [],
      "source": [
        "def create_graph_list(k):\n",
        "\n",
        "    X,Y=energyflow.qg_jets.load(num_data=100000, pad=True, ncol=4, generator='pythia',with_bc=False, cache_dir='~/.energyflow')     #loading the QG dataset X is 3-D array and Y is 1-D\n",
        "\n",
        "    data_list=[]\n",
        "\n",
        "    for i in range(X.shape[0]):                                      #Iterating through the jets to create a graph for each jet\n",
        "        x=torch.tensor(X[i], dtype=torch.float)                      # shape: (M, d)\n",
        "        x=(x-x.mean(dim=0))/x.std(dim=0)                             #Normalizing the features across particles (for each feature column).\n",
        "        mask=(x.abs().sum(dim=1)>1e-8)                               #Removing padded particles (all input features 0)\n",
        "        x=x[mask]\n",
        "        edge_index=knn_graph(x,k=k,loop=False)                       #Finds k-NN for every node/particle in the graph/jet in feature space (uses Euclidean distance)\n",
        "        label=torch.tensor([Y[i]],dtype=torch.long)                  #I am wrapping the label as a tensor as well to add to the PyG data object\n",
        "        data_list.append(Data(x=x,edge_index=edge_index,y=label))    #creating a PyG data object and appending it to a list of graphs. Each graph/jet is a PyG object\n",
        "\n",
        "    return data_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SrMIYYO9dT_"
      },
      "source": [
        "**GAT:**\n",
        "\n",
        "GAT layers learn to weigh each neighbor’s influence differently by computing attention scores. Multiple attention heads allow the model to capture different types of interactions. This is why I have chose GAT as one of my architectures as not all particles contribute equally to identifying a jet as quark- or gluon-initiated. GAT’s attention mechanism lets the network focus more on the most informative particle interactions. The heads enable the network to learn various aspects of the relationships between particles, which is crucial given the complex structure of jets.\n",
        "\n",
        "\n",
        "After processing through three GAT layers, I have used global mean pooling to aggregate node features into a single graph-level feature, which is then fed into a fully connected layer for classification.\n",
        "\n",
        "Below is the implementation of the GAT. Please run the cell so that it can be used later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "A2wQgUxT8jH8",
        "outputId": "9eea2b6d-eb04-45c8-92e0-b3f85160fd76"
      },
      "outputs": [],
      "source": [
        "class QG_GAT(nn.Module):\n",
        "\n",
        "  def __init__(self, in_channels, hidden_channels, out_channels=2, heads=4):\n",
        "\n",
        "    super().__init__()\n",
        "    #3 processing layers\n",
        "    self.layer_1=GATConv(in_channels, hidden_channels, heads=heads, concat=True)             #'heads' attention heads and their outputs concatenated. Outputs are feature vectors of hidden channels\n",
        "    self.layer_2=GATConv(hidden_channels*heads, hidden_channels, heads=heads, concat=True)   #Thus input channels is hidden_channels*heads. Same heads, concatenation and output feature vector size as as before\n",
        "    self.layer_3=GATConv(hidden_channels*heads, hidden_channels, heads=1, concat=False)      #1 attention head only now therefore the fc layer gets a feature vector of hidden_channels size\n",
        "\n",
        "    self.fc=nn.Linear(hidden_channels, out_channels)                                         #maps to final outputs\n",
        "\n",
        "  def forward(self, x, edge_index, batch):\n",
        "\n",
        "    x=functional.elu(self.layer_1(x, edge_index))                                            #Applying GAT layers with ELU activation.\n",
        "    x=functional.elu(self.layer_2(x, edge_index))\n",
        "    x=functional.elu(self.layer_3(x, edge_index))\n",
        "\n",
        "    x=global_mean_pool(x, batch)\n",
        "\n",
        "    return self.fc(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu-MHVwe_QiR"
      },
      "source": [
        "**DGCNN-**\n",
        "\n",
        "Instead of using a fixed graph, DGCNN recomputes the graph (using k-NN) at every layer. This means that after each layer, as the node features change, the graph’s connectivity is updated accordingly. Thus, I used DGCNN the network learns, the optimal relationships between particles can change. DGCNN’s dynamic graph construction allows the network to update these relationships at every layer, capturing higher-level, context-dependent interactions.\n",
        "\n",
        "DGCNN uses EdgeConv layers that consider both a node and its neighbors. EdgeConv layers are particularly good at learning local structures, which is important because the spatial and energy distributions of particles in a jet are key to distinguishing between quark and gluon jets.\n",
        "\n",
        "After several EdgeConv layers, global pooling (both max and mean) aggregates the node features into a graph-level feature vector, which is then passed through a fully connected layer to yield class scores.\n",
        "\n",
        "\n",
        "Below is the implementation of the DGCNN. Please run the cell so that it can be used later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Oo8CTrVkV1jm"
      },
      "outputs": [],
      "source": [
        "class QG_DGCNN(nn.Module):\n",
        "\n",
        "  def __init__(self, in_channels, hidden_channels, out_channels=2, k=16):\n",
        "\n",
        "    super().__init__()\n",
        "    self.k=k\n",
        "    #4 processing layers\n",
        "    self.conv_layer_1=EdgeConv(nn.Sequential(nn.Linear(2*in_channels, hidden_channels),nn.ReLU(), nn.Linear(hidden_channels,hidden_channels)))            #Input dimension is 2*in_channels because features of a node and its neighbor are concatenated. Outputs a feature vector of size hidden channels\n",
        "    self.conv_layer_2=EdgeConv(nn.Sequential(nn.Linear(2*hidden_channels, hidden_channels), nn.ReLU(), nn.Linear(hidden_channels,hidden_channels)))       #Input dimension is 2*hidden_channels (from previous layer's output)\n",
        "    self.conv_layer_3=EdgeConv(nn.Sequential(nn.Linear(2*hidden_channels, hidden_channels), nn.ReLU(), nn.Linear(hidden_channels,hidden_channels)))\n",
        "    self.conv_layer_4=EdgeConv(nn.Sequential(nn.Linear(2*hidden_channels, hidden_channels), nn.ReLU(), nn.Linear(hidden_channels,hidden_channels)))\n",
        "\n",
        "    self.conv_layer_list=[self.conv_layer_1, self.conv_layer_2, self.conv_layer_3, self.conv_layer_4]                                #Save all EdgeConv layers in a list for iteration in the forward pass\n",
        "\n",
        "    self.fc=nn.Linear(hidden_channels*2, out_channels)                                                                               #Takes the concatenated output from global max and mean pooling (2*hidden_channels) and maps it to out_channels classes\n",
        "\n",
        "\n",
        "  def forward(self, x, edge_index, batch):\n",
        "\n",
        "    for layer in self.conv_layer_list:\n",
        "      edge_index = knn_graph(x, k=self.k, batch=batch, loop=False)                 #Dynamically computing the k-NN graph based on the current node features\n",
        "      x=layer(x, edge_index)                                                       #Updating the node features using the current EdgeConv layer\n",
        "\n",
        "    x_max=global_max_pool(x, batch)\n",
        "    x_mean=global_mean_pool(x, batch)\n",
        "    x=torch.cat([x_max, x_mean], dim=1)                                            #Concatenating both pooled representations to form a graph-level feature vector\n",
        "\n",
        "    return self.fc(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-C_rh9O6CIBe"
      },
      "source": [
        "Below are functions for training and testing. These are used for a single epoch and are iteratively run later in main() for multiple (20) epochs\n",
        "\n",
        "Please run the cell so that they can be used later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SBqpG_CsV1R-"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, loader, optimizer, device):\n",
        "    \n",
        "    model.train()                                                 #Putting model in train mode\n",
        "    total_loss=0\n",
        "    for batch in loader:                                          #going through the batches here\n",
        "        batch=batch.to(device)\n",
        "        optimizer.zero_grad()                                     #I am clearing all previous gradient computations here\n",
        "        forward=model(batch.x, batch.edge_index, batch.batch)     #Running the forward method in the given model here\n",
        "        loss=functional.cross_entropy(forward, batch.y)           #Calculating cross entropy loss\n",
        "        loss.backward()                                           #Backpropagation\n",
        "        optimizer.step()                                          #Updating the parameters\n",
        "        total_loss+=loss.item()*batch.num_graphs                  #Loss for each batch is weighted with the number of graphs in each batch\n",
        "    return total_loss/len(loader.dataset)                         #Returns the average loss per graph/jet for an epoch\n",
        "\n",
        "def test_epoch(model, loader, device):\n",
        "\n",
        "    model.eval()                                                  #Putting model in evaluation mode\n",
        "    correct=0\n",
        "    for batch in loader:\n",
        "        batch=batch.to(device)\n",
        "        with torch.no_grad():                                     #Disabling gradient computation for evaluation\n",
        "            forward=model(batch.x, batch.edge_index, batch.batch) #Running the forward method in the model here\n",
        "            prediction=forward.argmax(dim=1)                      #Predicted class\n",
        "            correct+=prediction.eq(batch.y).sum().item()          #Number of correctly predicted graphs\n",
        "    return correct/len(loader.dataset)                            #Returning accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_dZXGRaDJDN"
      },
      "source": [
        "Below is the main functions that trains and tests both these models on 'num_epochs' epochs. I am using 20 as num_epochs when I call main.\n",
        "\n",
        "Please run the cell to train, test and generate best accuracy values when predicting with both models.\n",
        "\n",
        "I only split the dataset into training and test sets for simplicity, but a validation set can be put in as well with a different split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "in7A8REnV7BC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest DGCNN Accuracy:\u001b[39m\u001b[33m\"\u001b[39m, dgcnn_best_accuracy)\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m==\u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(num_epochs)\u001b[39m\n\u001b[32m     19\u001b[39m gat_best_accuracy, dgcnn_best_accuracy=\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m                                         \u001b[38;5;66;03m#Starting Training and Testing here over num_epochs\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     gat_train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgat_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgat_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m     gat_train_accuracy = test_epoch(gat_model, train_loader, device)\n\u001b[32m     25\u001b[39m     gat_test_accuracy = test_epoch(gat_model, test_loader, device)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, loader, optimizer, device)\u001b[39m\n\u001b[32m      8\u001b[39m forward=model(batch.x, batch.edge_index, batch.batch)     \u001b[38;5;66;03m#Running the forward method in the given model here\u001b[39;00m\n\u001b[32m      9\u001b[39m loss=functional.cross_entropy(forward, batch.y)           \u001b[38;5;66;03m#Calculating cross entropy loss\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m                                           \u001b[38;5;66;03m#Backpropagation\u001b[39;00m\n\u001b[32m     11\u001b[39m optimizer.step()                                          \u001b[38;5;66;03m#Updating the parameters\u001b[39;00m\n\u001b[32m     12\u001b[39m total_loss+=loss.item()*batch.num_graphs                  \u001b[38;5;66;03m#Loss for each batch is weighted with the number of graphs in each batch\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/qml-hep/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/qml-hep/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/qml-hep/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "def main(num_epochs):\n",
        "\n",
        "    device=torch.device('cpu' if torch.backends.mps.is_available() else 'mps')          #Checking if MPS (as I am training on a MacBook Pro) is available and using it if it is. Otherwise using CPU\n",
        "    print(device) #Can be used for checking what device being used for training\n",
        "    data_list=create_graph_list(k=16)                                                   #I am using k as 16 for k-NN\n",
        "    train_data,test_data=train_test_split(data_list, test_size=0.2, random_state=42)    #I have used a 80:20 split for training and testing data\n",
        "\n",
        "    train_loader=DataLoader(train_data, batch_size=32, shuffle=True)                    #I am doing batch processing here\n",
        "    test_loader=DataLoader(test_data, batch_size=32, shuffle=False)                     #Each Batch contains 32 graphs/jets and training data batches have been shuffled\n",
        "\n",
        "    in_channels=data_list[0].x.shape[1]                                                 #Number of input freatures per particle/node (i.e. d in (N,M,d))\n",
        "\n",
        "    gat_model=QG_GAT(in_channels, hidden_channels=64, out_channels=2, heads=4).to(device)   #Instance of our Quark Gluon GAT Classifier\n",
        "    dgcnn_model=QG_DGCNN(in_channels, hidden_channels=64, out_channels=2, k=16).to(device)  #Instance of the DGCNN Classifier\n",
        "\n",
        "    gat_optimizer=torch.optim.Adam(gat_model.parameters(), lr=1e-3)                            #I am using ADAM for training with 10^-3 learning rate\n",
        "    dgcnn_optimizer=torch.optim.Adam(dgcnn_model.parameters(), lr=1e-3)\n",
        "\n",
        "    gat_best_accuracy, dgcnn_best_accuracy=0,0                                         #Starting Training and Testing here over num_epochs\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        gat_train_loss = train_epoch(gat_model, train_loader, gat_optimizer, device)\n",
        "        gat_train_accuracy = test_epoch(gat_model, train_loader, device)\n",
        "        gat_test_accuracy = test_epoch(gat_model, test_loader, device)\n",
        "\n",
        "        dgcnn_train_loss = train_epoch(dgcnn_model, train_loader, dgcnn_optimizer, device)\n",
        "        dgcnn_train_accuracy = test_epoch(dgcnn_model, train_loader, device)\n",
        "        dgcnn_test_accuracy = test_epoch(dgcnn_model, test_loader, device)\n",
        "\n",
        "        if gat_test_accuracy>gat_best_accuracy:\n",
        "            gat_best_accuracy=gat_test_accuracy\n",
        "            torch.save(gat_model.state_dict(), 'best_gat_model.pt')                    #Saving the best model parameters for later use. I can load into the model if I wish to\n",
        "\n",
        "        if dgcnn_test_accuracy>dgcnn_best_accuracy:\n",
        "            dgcnn_best_accuracy=dgcnn_test_accuracy\n",
        "            torch.save(dgcnn_model.state_dict(), 'best_dgcnn_model.pt')\n",
        "\n",
        "        print(f\"Epoch: {epoch+1:02d}\")\n",
        "        print(f\"GAT Loss: {gat_train_loss:.4f}, GAT Train Accuaracy: {gat_train_accuracy:.4f}, GAT Test Accuracy: {gat_test_accuracy:.4f}\")\n",
        "        print(f\"DGCNN Loss: {dgcnn_train_loss:.4f}, DGCNN Train Accuaracy: {dgcnn_train_accuracy:.4f}, DGCNN Test Accuracy: {dgcnn_test_accuracy:.4f}\")\n",
        "        print()\n",
        "\n",
        "    print(f\"\\nTraining complete\")\n",
        "    print(\"Best GAT Accuracy:\", gat_best_accuracy)\n",
        "    print(\"Best DGCNN Accuracy:\", dgcnn_best_accuracy)\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    main(num_epochs=20)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "qml-hep",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
